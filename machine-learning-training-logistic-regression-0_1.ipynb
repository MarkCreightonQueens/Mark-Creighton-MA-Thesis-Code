{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Training Logistic Regression 0_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XoukRVly3T2Z0AwE_Muh_XRi8Q-zouGY",
      "authorship_tag": "ABX9TyOPfCipbzl+KYufMcxGMRVF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/MarkCreightonQueens/676838c6e8e3e922638118b79b4c0e43/machine-learning-training-logistic-regression-0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KT76lAf16TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as r\n",
        "r.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xKjh1wnV859",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b21e19bb-9885-46fa-d8bc-814d4fcf8714"
      },
      "source": [
        "#installing tools\n",
        "!pip install transformers \n",
        "!pip install wget\n",
        "!pip install tika\n",
        "!pip install ijson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6dc64f10dd3e44c0d520e59c6039f6db0458da21b6c55a710a448012511dffa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=d6874032ae0fb8da23e6c9941d1c30891c406df8b2c65c951eaa6ae4f0202c30\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (47.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=73b9b393cfde13d98bc44ae5eab149bdbdaed63e929b8ce51e8f9659ba0b03d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n",
            "Collecting ijson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/0b/e406ef9de26b2045f2ba689624d1844d1d8ffae991e0d2e792fde256e9ed/ijson-3.1.post0-cp36-cp36m-manylinux1_x86_64.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.1.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-iavskzXabH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2d406556-054b-4dab-ceda-29384c32d9d0"
      },
      "source": [
        "#Importing tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tika import parser\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import wget\n",
        "import urllib.request\n",
        "import re\n",
        "import gzip\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import json\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m_8dieYXzdJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d056a5b3-8e75-433a-ff11-23a68a6c8056"
      },
      "source": [
        "#get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at:{}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA3QLek1X1HN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "469f50d5-ffd5-43e9-ebfe-962ef47e683c"
      },
      "source": [
        "#Determining what GPU is available for use\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print ('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else: \n",
        "  print('No GPU available, using CPU instead.')\n",
        "  device = torch.device(\"CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NigBSA7nzbXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.unpack_archive('/content/drive/My Drive/Thesis/yelp_review_full_csv.tgz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urP9qrUcYj2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "9cff218e-93c7-4637-f5a7-ced8336c0c4c"
      },
      "source": [
        "#Reading in the CSV. \n",
        "\n",
        "data_train_full = pd.read_csv('/content/yelp_review_full_csv/train.csv', header = None)\n",
        "data_test = pd.read_csv('/content/yelp_review_full_csv/test.csv', header = None)\n",
        "\n",
        "print(data_train_full.shape)\n",
        "print(data_test.shape)\n",
        "\n",
        "#Creating a smaller subset of the training dataset due to computational restrictions\n",
        "data_train_1 = data_train_full[data_train_full[0] == 1].sample(50000, random_state=0)\n",
        "data_train_2 = data_train_full[data_train_full[0] == 2].sample(50000, random_state=0)\n",
        "data_train_3 = data_train_full[data_train_full[0] == 3].sample(50000, random_state=0)\n",
        "data_train_4 = data_train_full[data_train_full[0] == 4].sample(50000, random_state=0)\n",
        "data_train_5 = data_train_full[data_train_full[0] == 5].sample(50000, random_state=0)\n",
        "frames = [data_train_1, data_train_2, data_train_3, data_train_4, data_train_5]\n",
        "data_train = pd.concat(frames)\n",
        "#randomizing the order of the observations\n",
        "data_train = data_train.sample(frac=1, random_state=0)\n",
        "\n",
        "print(data_train[0].unique())\n",
        "print(data_train.shape)\n",
        "\n",
        "\n",
        "#Reassigning values so that 1-3 = 0 and 4-5 = 1\n",
        "data_train[0] = data_train[0].replace(1,0)\n",
        "data_train[0] = data_train[0].replace(2,0)\n",
        "data_train[0] = data_train[0].replace(3,0)\n",
        "data_train[0] = data_train[0].replace(4,1)\n",
        "data_train[0] = data_train[0].replace(5,1)\n",
        "\n",
        "data_test[0] = data_test[0].replace(1,0)\n",
        "data_test[0] = data_test[0].replace(2,0)\n",
        "data_test[0] = data_test[0].replace(3,0)\n",
        "data_test[0] = data_test[0].replace(4,1)\n",
        "data_test[0] = data_test[0].replace(5,1)\n",
        "\n",
        "print(data_train[0].unique())\n",
        "data_train = data_train.rename(columns={0:'rating',1:'text'})\n",
        "data_test = data_test.rename(columns={0:'rating',1:'text'})\n",
        "data_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(650000, 2)\n",
            "(50000, 2)\n",
            "[3 2 4 1 5]\n",
            "(250000, 2)\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I got 'new' tires from them and within two wee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Don't waste your time.  We had two different p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>All I can say is the worst! We were the only 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>I have been to this restaurant twice and was d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>0</td>\n",
              "      <td>Just wanted to write a review to chip in with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>1</td>\n",
              "      <td>Great ambience. Great drinks. Great food. I lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>1</td>\n",
              "      <td>I have been to the other Monks locations so I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>0</td>\n",
              "      <td>Don't go here.  I know you might want to try i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>0</td>\n",
              "      <td>Buffet was recently open after renovation so m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       rating                                               text\n",
              "0           0  I got 'new' tires from them and within two wee...\n",
              "1           0  Don't waste your time.  We had two different p...\n",
              "2           0  All I can say is the worst! We were the only 2...\n",
              "3           0  I have been to this restaurant twice and was d...\n",
              "4           0  Food was NOT GOOD at all! My husband & I ate h...\n",
              "...       ...                                                ...\n",
              "49995       0  Just wanted to write a review to chip in with ...\n",
              "49996       1  Great ambience. Great drinks. Great food. I lo...\n",
              "49997       1  I have been to the other Monks locations so I ...\n",
              "49998       0  Don't go here.  I know you might want to try i...\n",
              "49999       0  Buffet was recently open after renovation so m...\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZeRQiIv0Hj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd3c1995-76cc-4c78-ea2c-376c3c1636b3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "LR_0_1_data = pd.read_csv('/content/drive/My Drive/Thesis/Output_backup.csv')\n",
        "print(LR_0_1_data.shape)\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(data_train['text'])\n",
        "X = cv.transform(data_train['text'])\n",
        "X_test = cv.transform(data_test['text'])\n",
        "LR_0_1_vector = cv.transform(LR_0_1_data['Sentences'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42083, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmVDfFWNeRKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = pd.Series(data_train['rating'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFL5Q4jPaI5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0d8031b1-0500-47ef-e702-4c6773332443"
      },
      "source": [
        "model = LogisticRegression(C=0.25, max_iter = 1024)\n",
        "model.fit(X, target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.25, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1024,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5yl9rAihgzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_test = pd.Series(data_test['rating'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rjkZT19hgCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cce5588c-26aa-4336-f475-f94935815207"
      },
      "source": [
        "print (\"Final Accuracy:\", accuracy_score(target_test, model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.87044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeTRP7mRahbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f785fbf5-44a4-4cc3-f267-f696ddf8f2e7"
      },
      "source": [
        "\n",
        "feature_to_coef = {\n",
        "    word: coef for word, coef in zip(\n",
        "        cv.get_feature_names(), model.coef_[0]\n",
        "    )\n",
        "}\n",
        "for best_positive in sorted(\n",
        "    feature_to_coef.items(), \n",
        "    key=lambda x: x[1], \n",
        "    reverse=True)[:5]:\n",
        "    print (best_positive)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('hooked', 1.2991881492345838)\n",
            "('perfection', 1.2376714855481667)\n",
            "('nkudos', 1.2352296582264102)\n",
            "('gem', 1.162726580540949)\n",
            "('phenomenal', 1.1445424829896507)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2NhkNkPaqxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b2768adf-052b-4d20-ed06-e1158e292471"
      },
      "source": [
        "for best_negative in sorted(\n",
        "    feature_to_coef.items(), \n",
        "    key=lambda x: x[1])[:5]:\n",
        "    print (best_negative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('worst', -1.9637484304564596)\n",
            "('underwhelmed', -1.9595432267689843)\n",
            "('poisoning', -1.8859964640562503)\n",
            "('mediocre', -1.811066967819704)\n",
            "('horrible', -1.7416602260753213)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tkSQh0xzKLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cd85802-90d0-4c9a-ca30-79ca94cfd940"
      },
      "source": [
        "LR_0_1_results = model.predict(LR_0_1_vector)\n",
        "print(LR_0_1_results.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42083,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OvHNv-tzKC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8ea22cf6-4fbd-4625-a74a-dc7eca7f3231"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Thesis/Logistic Regression')\n",
        "LR_0_1_results = pd.DataFrame(data = LR_0_1_results)\n",
        "outfile = 'LR_0_1_results.csv'\n",
        "LR_0_1_results.to_csv(outfile)\n",
        "LR_0_1_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42078</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42079</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42080</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42081</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42082</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42083 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0\n",
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "...   ..\n",
              "42078  0\n",
              "42079  0\n",
              "42080  1\n",
              "42081  0\n",
              "42082  0\n",
              "\n",
              "[42083 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW09H_GRHHVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "os.chdir('/content/drive/My Drive/Thesis/Logistic Regression')\n",
        "filename = 'Logistic Regression 0_1'\n",
        "pickle.dump(model, open(filename,'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}