{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1X2FfStE6_05B7AlYIceVYLLj0Vl4s9w0",
      "authorship_tag": "ABX9TyMXOsR4HvAZ7qAL+QdoOjCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b9a4d8e4aa9487a9e9f1d06125fb133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db83502c033f48ae91a70f83b0516aeb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9aa6d058206d48abb3fd116bcfce801e",
              "IPY_MODEL_da216ab231f14789979b62c9e968936a"
            ]
          }
        },
        "db83502c033f48ae91a70f83b0516aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9aa6d058206d48abb3fd116bcfce801e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_450ca734a540462f871d6e4cabf49548",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c306b36052874d2b9848b4ff659962eb"
          }
        },
        "da216ab231f14789979b62c9e968936a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_006b82eaf4eb4fa99951c6c0f5451189",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03f04909946d490ea4451c6932192800"
          }
        },
        "450ca734a540462f871d6e4cabf49548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c306b36052874d2b9848b4ff659962eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "006b82eaf4eb4fa99951c6c0f5451189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03f04909946d490ea4451c6932192800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/MarkCreightonQueens/c1875f4429be64b4e6ca52a36bd3e263/machine-learning-data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlL3SQaoS3_l",
        "colab_type": "text"
      },
      "source": [
        "Machine Learning Data:\n",
        "\n",
        "Goal is to input relevant FOMC documentation and otbtain text data/sentences that are readable by machine learning algorithms. \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctQr7jLGC42X",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the environment\n",
        "\n",
        "Import of needed tools\n",
        "\n",
        "Import of data\n",
        "\n",
        "Toggling on the GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkRH6IULSwyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2750336d-a9b6-432b-d9ac-1f9ac5055ce7"
      },
      "source": [
        "#installing tools\n",
        "!pip install transformers \n",
        "!pip install wget\n",
        "!pip install tika\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 20.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e96703c2e53da96fd717afdd959b3549d90183a990e1e782aa3031c2188ecde0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=40db3ee44925d2296ac68110a8d8fc49149d80ba4e185d3cbbfb59712d55e978\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (49.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=e041ea6f185af9a5b042b89875b37879c6ae04c0e7022b4f4eaacf1bc2ca0570\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLUDKtJgESzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "1b9a4d8e4aa9487a9e9f1d06125fb133",
            "db83502c033f48ae91a70f83b0516aeb",
            "9aa6d058206d48abb3fd116bcfce801e",
            "da216ab231f14789979b62c9e968936a",
            "450ca734a540462f871d6e4cabf49548",
            "c306b36052874d2b9848b4ff659962eb",
            "006b82eaf4eb4fa99951c6c0f5451189",
            "03f04909946d490ea4451c6932192800"
          ]
        },
        "outputId": "36aed5d6-af2b-4eb8-a758-211edca7cee4"
      },
      "source": [
        "#Importing tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tika import parser\n",
        "#Note: need to run older version of tensor flor for BERT to work. \n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import wget\n",
        "import urllib.request\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "import datetime\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b9a4d8e4aa9487a9e9f1d06125fb133",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_wB7BpXHh7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08ff88b8-2e8e-4954-a3f7-8ab0f7fe0e64"
      },
      "source": [
        "#get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at:{}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at:/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuyxNxiXHwWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8892c35c-e8df-471c-d164-42efb8de8b9c"
      },
      "source": [
        "#Determining what GPU is available for use\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print ('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else: \n",
        "  print('No GPU available, using CPU instead.')\n",
        "  device = torch.device(\"CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuiK7WK2FW31",
        "colab_type": "text"
      },
      "source": [
        "# Data Import\n",
        "\n",
        "Importing FOMC meeting minutes and transcripts\n",
        "\n",
        "TODO: get Amazon data for training purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaggLwRyLrXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9bedd5e-3462-4f2a-9704-4a0d031ad789"
      },
      "source": [
        "#Extracting the data from the zip file\n",
        "shutil.unpack_archive('/content/drive/My Drive/Thesis/FOMC Documentation.zip')\n",
        "print('Complete.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szJA0sG5FZi9",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning \n",
        "\n",
        "Part 1: Cleaning the minutes\n",
        "\n",
        "Note: in 2007 the minute format changed, as a result of this two separate approaches are necessary to obtain the same final result\n",
        "\n",
        "Part 2: Cleaning the transcripts\n",
        "\n",
        "This may be somewhat involved..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x32WeKbuUipP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61acc659-f57c-4cad-b2c8-d017118f53d3"
      },
      "source": [
        "#Declaring functions that will be used to clean the data. \n",
        "\n",
        "#remove leading space in entries to allow for easier combination/interpretation\n",
        "#combined if ends in not '.' and starts with month.  \n",
        "\n",
        "\n",
        "#function to remove empty entries in the text array\n",
        "def remove_empties(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        # This if statement will see spaces as empty, i.e ' ' will be removed\n",
        "        if string.strip():\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#function to remove 1 character strings.\n",
        "\n",
        "\n",
        "#function to remove text break new line notation in the text array \n",
        " \n",
        "def remove_line_breaks(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if '-\\n' in string:\n",
        "            output.append(string.replace('-\\n',''))\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        " \n",
        "#function to remove new line notation in the text array\n",
        "def remove_whitespace(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if string != re.sub('\\n','', string):\n",
        "            output.append(re.sub('\\n','', string))\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#Functions to recombine paragraphs that were erroneously separated\n",
        "def combine_erroneous_separations_if_word_break(string_list):\n",
        "    output = []\n",
        "    string_list = string_list.copy()\n",
        "    while string_list:\n",
        "        #Pops the first string off the list\n",
        "        first_string = string_list.pop(0)\n",
        "        if first_string[-1] == '-':\n",
        "            #Pops the next string off\n",
        "            second_string = string_list.pop(0)\n",
        "            #Concatenates them together, removing the end dash\n",
        "            output.append(first_string[:-1] + second_string)\n",
        "            continue\n",
        "        else:\n",
        "            output.append(first_string)\n",
        "    return output\n",
        "\n",
        "#TODO: Double check this, some paragraphs are not being recombined despite meeting the criteria AFAIK\n",
        "#Function to recombine erroneously separated paragraphs due to page breaks\n",
        "def combine_erroneous_separations_if_page_break(string_list):\n",
        "    output = []\n",
        "    string_list = string_list.copy()\n",
        "    while string_list:\n",
        "        #Pops the first string off the list\n",
        "        first_string = string_list.pop(0)\n",
        "        if first_string[-1] != '.' and string_list:\n",
        "            #Pops the next string off\n",
        "            second_string = string_list.pop(0)\n",
        "            if second_string[0].islower() or second_string[0] == 'I' or second_string[0:6] == 'Reserve':\n",
        "            #Concatenates them together, removing the end dash\n",
        "                    results = first_string +' '+second_string\n",
        "                    output.append(results)\n",
        "                    continue\n",
        "            output.append(first_string)\n",
        "            output.append(second_string)\n",
        "        else:\n",
        "            output.append(first_string)\n",
        "    return output\n",
        "\n",
        "#function to remove double spaces\n",
        "def remove_double_spaces(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        results1 = string\n",
        "        results2 = re.sub('  ',' ', string)\n",
        "        if results1 != results2:\n",
        "            while results1 != results2:\n",
        "                results1 = re.sub('  ',' ', results1)\n",
        "                results2 = re.sub('  ',' ', results2)\n",
        "                if results1 == results2:\n",
        "                    output.append(results1)\n",
        "        else:\n",
        "            output.append(results1)\n",
        "    return output\n",
        "\n",
        "#remove space at the end of the entry\n",
        "def remove_end_spaces(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if string[-1] == ' ':\n",
        "            string = string[0:-1]\n",
        "        output.append(string)\n",
        "    return output\n",
        "\n",
        "#re-add apostrophes\n",
        "def add_apostrophes(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        results = string\n",
        "        results = re.sub('â€œ','\\\"', results)\n",
        "        results = re.sub('â€™','\\'', results)\n",
        "        output.append(results) \n",
        "    return output\n",
        "\n",
        "#Remove page references. Note: Run remove_double_spaces\n",
        "def remove_page_references(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        results = re.sub(r'Page \\d+','', string)\n",
        "        output.append(results) \n",
        "    return output\n",
        "\n",
        "#Remove FOMC formatting\n",
        "def remove_FOMC_formatting(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        \n",
        "        #There is no easy way to detext these without risking the elimination of useful information, so I'm using an exhaustive approach to avoid false positives. \n",
        "        results = string\n",
        "        #Single day events\n",
        "        #Full spelling\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee January \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee February \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee March \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee April \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee May \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee June \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee July \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee August \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee September \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee October \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee November \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee December \\d+, \\d+','', results)\n",
        "        #Full spelling with comma\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, January \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, February \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, March \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, April \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, May \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, June \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, July \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, August \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, September \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, October \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, November \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, December \\d+, \\d+','', results)   \n",
        "        #Alternative spelling\n",
        "        results = re.sub('Minutes of Federal Open Market Committee October 30-31, 2007','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee January \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee February \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee March \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee April \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee May \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee June \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee July \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee August \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee September \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee October \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee November \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee December \\d+, \\d+','', results)\n",
        "        #Alternative spelling with commas\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October 30-31, 2007','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, January \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, February \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, March \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, April \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, May \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, June \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, July \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, August \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, September \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, November \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, December \\d+, \\d+','', results)\n",
        "        #Minutes of the meeting\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October 30-31, 2007','', results)\n",
        "        results = re.sub('Minutes of the Meeting of January \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of February \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of March \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of April \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of May \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of June \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of July \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of August \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of September \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of October \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of November \\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of December \\d+, \\d+','', results)       \n",
        "        #FOMC spelling\n",
        "        results = re.sub('FOMC Minutes, October 30-31, 2007','', results)\n",
        "        results = re.sub('FOMC Minutes, January \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, February \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, March \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, April \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, May \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, June \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, July \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, August \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, September \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, October \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, November \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, December \\d+, \\d+','', results)\n",
        "        #FOMC for spelling\n",
        "        results = re.sub('FOMC Minutes for October 30-31, 2007','', results)\n",
        "        results = re.sub('FOMC Minutes for January \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for February \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for March \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for April \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for May \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for June \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for July \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for August \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for September \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for October \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for November \\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for December \\d+, \\d+','', results)\n",
        "\n",
        "        #Multi day events\n",
        "        #Full spelling\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee December \\d+\\-\\d+, \\d+','', results)\n",
        "        #Full spelling with comma\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Federal Open Market Committee, December \\d+\\-\\d+, \\d+','', results)   \n",
        "        #Alternative spelling\n",
        "        results = re.sub('Minutes of Federal Open Market Committee October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee December \\d+\\-\\d+, \\d+','', results)\n",
        "        #Alternative spelling with commas\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, December \\d+\\-\\d+, \\d+','', results)\n",
        "        #Minutes of the meeting\n",
        "        results = re.sub('Minutes of Federal Open Market Committee, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('Minutes of the Meeting of December \\d+\\-\\d+, \\d+','', results)       \n",
        "        #FOMC spelling\n",
        "        results = re.sub('FOMC Minutes, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes, December \\d+\\-\\d+, \\d+','', results)\n",
        "        #FOMC for spelling\n",
        "        results = re.sub('FOMC Minutes for October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for January \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for February \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for March \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for April \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for May \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for June \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for July \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for August \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for September \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for October \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for November \\d+\\-\\d+, \\d+','', results)\n",
        "        results = re.sub('FOMC Minutes for December \\d+\\-\\d+, \\d+','', results) \n",
        "        #Old formatting\n",
        "        results = re.sub('$\\d+/\\d+/\\d+ FRB(.*?)\\d+/\\d+^','', results)\n",
        "        #6/17/2020 FRB: FOMC Minutes--January 31-February 1, 1995 https://www.federalreserve.gov/fomc/MINUTES/1995/19950201min.htm 1/14\n",
        "        output.append(results) \n",
        "    return output\n",
        "\n",
        "#Function to remove page reference\n",
        "def remove_page_references(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        results = string   \n",
        "        results = re.sub('Minutes of the Meeting of(.*?)Page \\d+','', results)\n",
        "        results = re.sub('Page \\d+(.*?)Federal Open Market Committee','', results)\n",
        "        \n",
        "        output.append(results) \n",
        "    return output\n",
        "\n",
        "#Function to remove entries comprised soley of digits\n",
        "def remove_digit_only_entries(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if re.match('^\\d+$', string):\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#Function to remove entries that correspond to d.dd.d\n",
        "def remove_d_dd_d_entries(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if re.match('^\\d\\.\\d+\\.\\d$', string):\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#Function to remove the formatting on tables that gets captured in the parser\n",
        "def remove_table_formatting(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if (re.match('^Number of participants$', string)) or (re.match('^Number of Participants$', string)) or (re.match('^Percentage range$', string)) or (re.match('^Percent range$', string)) or (re.match('^Longer run$', string)) or (re.match('^January projections$', string)) or (re.match('^February projections$', string)) or (re.match('^March projections$', string)) or (re.match('^April projections$', string)) or (re.match('^May projections$', string)) or (re.match('^June projections$', string)) or (re.match('^July projections$', string)) or (re.match('^August projections$', string)) or (re.match('^September projections$', string)) or (re.match('^October projections$', string)) or (re.match('^November projections$', string)) or (re.match('^December projections$', string))  or (re.match('^January Projections$', string)) or (re.match('^February Projections$', string)) or (re.match('^March Projections$', string)) or (re.match('^April Projections$', string)) or (re.match('^May Projections$', string)) or (re.match('^June Projections$', string)) or (re.match('^July Projections$', string)) or (re.match('^August Projections$', string)) or (re.match('^September Projections$', string)) or (re.match('^October Projections$', string)) or (re.match('^November Projections$', string)) or (re.match('^December Projections$', string)):\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "\n",
        "#Function ro remove strings that consist of a single character\n",
        "def remove_single_digit_strings(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if (re.match('^.$', string)):\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#Function to remove strings with no letter characters\n",
        "def remove_strings_without_letters(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if re.search('[a-zA-Z]', string):\n",
        "              output.append(string)\n",
        "        else:\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "    return output\n",
        "\n",
        "#Function to remove leading spaces in an entry\n",
        "#Note: Run this after removing double spaces\n",
        "def remove_leading_space(string_list):\n",
        "    output = []\n",
        "    string_list = string_list.copy()\n",
        "    for string in string_list:\n",
        "        if string[0] == ' ':\n",
        "            results = string[1:]\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "#Function to remove formatting from old style documents\n",
        "def remove_old_style_formatting(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        #There is no easy way to detext these without risking the elimination of useful information, so I'm using an exhaustive approach to avoid false positives. \n",
        "        results = string\n",
        "        #Single day events\n",
        "        #Full spelling\n",
        "        results = re.sub('^https(.*?)\\d$','', results)\n",
        "        output.append(results)\n",
        "    return output\n",
        "\n",
        "def remove_old_style_date_formatting(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        #There is no easy way to detext these without risking the elimination of useful information, so I'm using an exhaustive approach to avoid false positives. \n",
        "        results = string\n",
        "        #Single day events\n",
        "        #Full spelling\n",
        "        results = re.sub('^\\d+/\\d+/\\d+(.*?)FRB(.*?)\\d+$','', results)\n",
        "        output.append(results)\n",
        "    return output\n",
        "\n",
        "\n",
        "#Function to remove the formatting on tables that gets captured in the parser\n",
        "def remove_meta_data(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "        if (re.match('ASCII(.*?)False$', string) or re.match('CreateJDFFile(.*?)setpagedevice$', string)) :\n",
        "            results = ''\n",
        "            output.append(results)\n",
        "        else:\n",
        "            output.append(string)\n",
        "    return output\n",
        "\n",
        "\n",
        "print('Complete.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZQxkmM7SaRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Variables refrenced\n",
        "#Minimum length that a paragraph can take:\n",
        "#Selected to eliminate superfluous paragraphs that slip through the text cleaning. \n",
        "min_length_para = 8\n",
        "\n",
        "#Minimum length that a sentence can take\n",
        "#Selected to weed out stump stentences that have slipped through. \n",
        "min_length_sent = 2\n",
        "\n",
        "#Maximum length that a sentence can take\n",
        "#Selected due to model constaints, processing time increases exponentially as max length increases. \n",
        "\n",
        "max_length_sent = 128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJFbfQHLXI16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtaining the minutes information for the new format\n",
        "\n",
        "os.chdir('/content/FOMC Documentation/All Minutes')\n",
        "\n",
        "#Defines a function that obtains the location of the desired text and saves it\n",
        "def read_text(path):\n",
        "    f = open(path, \"r\")\n",
        "    return f.read()\n",
        "\n",
        "def get_file_paths():\n",
        "  return glob.glob('fomcminutes*.pdf')\n",
        "\n",
        "get_file_paths()\n",
        "\n",
        "#creates dataframe to store the text in for cleaning\n",
        "def create_entry(ID,path,raw_text):\n",
        "  return {'ID': ID, 'path': path, 'raw_text': raw_text}\n",
        "\n",
        "#filling the empty dataframe\n",
        "end_result = pd.DataFrame(columns=['ID','path','raw_text'])\n",
        "\n",
        "count = 0\n",
        "for path in get_file_paths():\n",
        "    #gets text\n",
        "    parsed = parser.from_file(path)\n",
        "    raw_text = parsed['content']\n",
        "\n",
        "    #Stores result as row = count\n",
        "    end_result.loc[count] = create_entry(count,path,raw_text)\n",
        "\n",
        "    #increments counter\n",
        "    count = count+1\n",
        "\n",
        "#sort the documents by date before applying functions\n",
        "\n",
        "#function to get the date string from the file path.\n",
        "def date_string(string_list):\n",
        "    output = []\n",
        "    for string in string_list:\n",
        "          results = re.findall('\\d+', string)\n",
        "          results = \"\".join(results)\n",
        "          output.append(results)\n",
        "    return output\n",
        "\n",
        "dates_temp = pd.Series(end_result['path'])\n",
        "\n",
        "dates_temp = dates_temp.apply(date_string)\n",
        "dates_temp = dates_temp.str.join(\"\")\n",
        "\n",
        "end_result['dates_temp'] = dates_temp\n",
        "\n",
        "\n",
        "#TODO: \n",
        "#function to reformat datestring to mm/dd/yyyy\n",
        "dates = []\n",
        "for string in dates_temp:\n",
        "    year = string[0:4]\n",
        "    month = string[4:6]\n",
        "    #if month[0] == '0':\n",
        "        #month = month[1]\n",
        "    day = string[6:8]\n",
        "    #if day[0] == '0':\n",
        "        #day = day[1]\n",
        "    results = month + '/' + day + '/' + year\n",
        "    dates.append(results)\n",
        "\n",
        "end_result['dates'] = dates\n",
        "\n",
        "end_result = end_result.sort_values(by = ['dates_temp'])\n",
        "\n",
        "end_result['text'] = end_result['raw_text'].str.split('\\n\\n')\n",
        "\n",
        "#testing\n",
        "test1 = end_result['raw_text']\n",
        "test2 = end_result['text']\n",
        "\n",
        "#print(test1.apply(lambda x : 'The recent information on inflation was seen'  in x ).sum() > 0)\n",
        "#print(test1.head())\n",
        "#print(test1.shape)\n",
        "test2 = test2.explode()\n",
        "test2 = test2.loc[test2 !='']\n",
        "test2 = test2.dropna()\n",
        "\n",
        "outfile = 'test2.csv'\n",
        "test2.to_csv(outfile)\n",
        "test3 = pd.read_csv('test2.csv')\n",
        "test3 = test3.dropna()\n",
        "test3 = pd.Series(test3['text'])\n",
        "test2 = test2.reset_index(drop = True)\n",
        "\n",
        "#print(test2.apply(lambda x : 'The recent information on inflation was seen'  in x ).sum() > 0)\n",
        "#print(test2.head())\n",
        "#print(test2.shape)\n",
        "#print(test3.apply(lambda x : 'The recent information on inflation was seen'  in x ).sum() > 0)\n",
        "#print(test3.head())\n",
        "#print(test3.shape) \n",
        "\n",
        "\n",
        "#is_equivalent = test2.eq(test3)\n",
        "#print(is_equivalent.sum())\n",
        "#\\testing\n",
        "\n",
        "end_result = end_result.set_index(end_result['dates_temp'], drop = True)\n",
        "\n",
        "end_result['ID'] = np.arange(len(end_result))\n",
        "\n",
        "text = pd.Series(end_result['text'])\n",
        "\n",
        "#testing\n",
        "\n",
        "#/testing\n",
        "\n",
        "#testing\n",
        "print('Before applying cleaning fuctions:')\n",
        "test4 = text\n",
        "test4 = test4.explode()\n",
        "test4 = test4.loc[test4!='']\n",
        "test4 = test4.reset_index(drop=True)\n",
        "print(test4.head())\n",
        "print(test4.shape)\n",
        "print(test4.apply(lambda x : 'The recent information on inflation was seen'  in x ).sum() > 0)\n",
        "\n",
        "#\\testing \n",
        "\n",
        "#applying the gaggle of functions\n",
        "text = text.apply(remove_digit_only_entries)\n",
        "text = text.apply(remove_empties)\n",
        "#TODO: combine if ends in not '.' and starts with month.  \n",
        "text = text.apply(remove_double_spaces)\n",
        "text = text.apply(remove_leading_space)\n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(combine_erroneous_separations_if_word_break)\n",
        "text = text.apply(remove_double_spaces)\n",
        "text = text.apply(remove_leading_space)\n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(combine_erroneous_separations_if_page_break) \n",
        "text = text.apply(remove_line_breaks)\n",
        "text = text.apply(remove_whitespace)\n",
        "text = text.apply(remove_page_references) \n",
        "text = text.apply(remove_digit_only_entries)\n",
        "text = text.apply(remove_FOMC_formatting)\n",
        "text = text.apply(remove_double_spaces)\n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(remove_end_spaces)\n",
        "text = text.apply(add_apostrophes)\n",
        "text = text.apply(remove_table_formatting)\n",
        "text = text.apply(remove_strings_without_letters)\n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(remove_double_spaces)\n",
        "text = text.apply(remove_leading_space)\n",
        "test = text.apply(remove_meta_data)\n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(combine_erroneous_separations_if_page_break) \n",
        "text = text.apply(combine_erroneous_separations_if_page_break) \n",
        "text = text.apply(combine_erroneous_separations_if_page_break) \n",
        "text = text.apply(combine_erroneous_separations_if_page_break) \n",
        "text = text.apply(remove_empties)\n",
        "text = text.apply(add_apostrophes)\n",
        "\n",
        "print(min(text, key = len))\n",
        "\n",
        "\n",
        "outfile = 'test3.csv'\n",
        "text.to_csv(outfile)\n",
        "\n",
        "\n",
        "#testing\n",
        "print('After applying cleaning functions:')\n",
        "test5 = text\n",
        "test5 = test5.explode()\n",
        "test5 = test5.loc[test5!='']\n",
        "test5 = test5.reset_index(drop=True)\n",
        "print(test5.head())\n",
        "print(test5.shape)\n",
        "print(test5.apply(lambda x : 'The recent information on inflation was seen'  in x ).sum() > 0)\n",
        "#\\testing\n",
        "\n",
        "end_result['text'] = text\n",
        "\n",
        "\n",
        "#Note: Do text split \"by unanimous vote\" here\n",
        "\n",
        "#Breaking down document to paragraphs\n",
        "Output = end_result.explode('text')\n",
        "Output = Output.rename(columns={'text':'Paragraphs'})\n",
        "\n",
        "#Defining bounds of relevant text\n",
        "Output = Output.reset_index(drop=True)\n",
        "Output['temp'] = Output['Paragraphs'].apply(lambda x: 'unanimous' in x or 'Unanimous' in x)\n",
        "temp = Output.loc[ Output['temp'] ].groupby('ID').first()\n",
        "results = Output.loc[Output['temp']].reset_index().groupby('ID').first().set_index('index')\n",
        "Output['Start flag'] = False\n",
        "Output['Start flag'].loc[results.index] = True\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "#TODO:\n",
        "#Figure out why this is dropping 2 meetings entirely. \n",
        "#happening in start flag. \n",
        "#dropping irrelevant paragraphs\n",
        "print('Shape before dropping Keep:',Output.shape)\n",
        "for i in range(1, len(Output)):\n",
        "    Output.loc[i,'Keep'] = Output.loc[i,'Start flag']\n",
        "for i in range(1, len(Output)): \n",
        "    if Output.loc[i-1,'Keep'] == True:\n",
        "        if Output.loc[i,'ID'] == Output.loc[i-1,'ID']:\n",
        "            Output.loc[i,'Keep'] = Output.loc[i-1,'Keep']\n",
        "\n",
        "Output = Output.reset_index(drop = True)\n",
        "Output = Output.drop(Output[Output.Keep == False].index)\n",
        "\n",
        "print('Shape after dropping Keep:',Output.shape)\n",
        "print('Unique Keeps:',Output['Keep'].nunique())\n",
        "print('Values of Keep:',Output['Keep'].unique())\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "\n",
        "\n",
        "#TODO: Add testing for this section\n",
        "\n",
        "#dropping short paragraphs\n",
        "Paragraphs_tokenized = []\n",
        "Paragraphs = pd.Series(Output['Paragraphs'])\n",
        "for para in Paragraphs:\n",
        "    results = tokenizer.encode(para, add_special_tokens=True)\n",
        "    Paragraphs_tokenized.append(results)\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "\n",
        "\n",
        "#Getting the length of the tokenized sentences\n",
        "#Note: this will be overwritten later on\n",
        "print('Shape before short paragraphs dropped:')\n",
        "print(Output.shape)\n",
        "Output['Paragraphs_tokenized'] = Paragraphs_tokenized\n",
        "Output['Paragraphs_tokenized_length'] = Output['Paragraphs_tokenized'].apply(lambda x: len(x))\n",
        "Output = Output.drop(Output[Output['Paragraphs_tokenized_length'] < min_length_para].index)\n",
        "print('Shape after short paragraphs dropped:')\n",
        "print(Output.shape)\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "\n",
        "#Assigns ID values to paragraphs\n",
        "Output['Meta Paragraph ID'] = np.arange(len(Output))\n",
        "Output['Paragraph ID'] = Output.groupby('ID').apply( lambda x: list(range(0,x.shape[0]))).explode().values\n",
        "\n",
        "#Breaking down document to sentences\n",
        "Output['Sentences'] = Output['Paragraphs'].apply(sent_tokenize)\n",
        "Output = Output.explode('Sentences')\n",
        "\n",
        "\n",
        "#Dropping short and long sentences.\n",
        "Sentences_tokenized = []\n",
        "Sentences = pd.Series(Output['Sentences'])\n",
        "for sent in Sentences:\n",
        "    results = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    Sentences_tokenized.append(results)\n",
        "\n",
        "#Getting the length of the tokenized sentences\n",
        "#Note: this will be overwritten later on\n",
        "print('Shape before short/long sentences dropped:')\n",
        "print(Output.shape)\n",
        "Output['Sentences_tokenized'] = Sentences_tokenized\n",
        "Output['Sentences_tokenized_length'] = Output['Sentences_tokenized'].apply(lambda x: len(x))\n",
        "Output = Output.drop(Output[Output['Sentences_tokenized_length'] < min_length_sent].index)\n",
        "Output = Output.drop(Output[Output['Sentences_tokenized_length'] > max_length_sent].index)\n",
        "print('Shape after short/long sentences dropped:')\n",
        "print(Output.shape)\n",
        "\n",
        "print(Output.shape)\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "\n",
        "#Dropping information no longer needed for analysis\n",
        "Output = Output.drop(['path','raw_text','Paragraphs_tokenized','Paragraphs_tokenized_length'],axis=1)\n",
        "\n",
        "#Assigns ID values to sentences\n",
        "Output['Meta Sentence ID'] = np.arange(len(Output))\n",
        "Output['Sentence ID'] = Output.groupby('ID').apply( lambda x: list(range(0,x.shape[0]))).explode().values\n",
        "\n",
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "#Testing\n",
        "print('After 1 digit strings dropped:')\n",
        "test7 = Output\n",
        "print(test7.head())\n",
        "print(test7.shape)\n",
        "#\\Testing\n",
        "test8 = pd.Series(Output['Sentences'])\n",
        "print(test8.apply(lambda x : 'Â'  in x ).sum() > 0)\n",
        "\n",
        "outfile = 'test8.csv'\n",
        "test8.to_csv(outfile)\n",
        "test9 = pd.read_csv('test8.csv')\n",
        "test9 = test9.dropna()\n",
        "test9 = pd.Series(test9['Sentences'])\n",
        "test8 = test8.reset_index(drop = True)\n",
        "\n",
        "print(test9.apply(lambda x : 'Â'  in x ).sum() > 0)\n",
        "#\\Testing\n",
        "\n",
        "print('Complete.')\n",
        "\n",
        "outfile = 'outfile.csv'\n",
        "Output.to_csv(outfile)\n",
        "print('Unique IDs:',Output['ID'].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfwrQGOuFmJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15216099-2736-4369-926e-ba84ff04c140"
      },
      "source": [
        "print('Unique IDs:',Output['ID'].nunique())\n",
        "os.chdir('/content/drive/My Drive/Thesis')\n",
        "outfile = 'Output_backup.csv'\n",
        "Output.to_csv(outfile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique IDs: 176\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}